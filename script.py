import os
import re
import ipaddress
import Levenshtein
from flask import Flask, request, jsonify
from bs4 import BeautifulSoup
from flask_sqlalchemy import SQLAlchemy

app = Flask(__name__)
app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///whitelist.db'
app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False

db = SQLAlchemy(app)


class WhitelistDomain(db.Model):
    __tablename__ = 'whitelist'

    id = db.Column(db.Integer, primary_key=True)
    rank = db.Column(db.Integer, nullable=True)
    domain = db.Column(db.String(255), unique=True, nullable=False)

    def __repr__(self):
        return f'<Domain {self.domain}>'


class BKTree:
    def __init__(self):
        self.tree = None

    def add(self, word):
        if self.tree is None:
            self.tree = (word, {})
            return
        node = self.tree
        while True:
            parent_word, children = node
            distance = Levenshtein.distance(word, parent_word)
            if distance == 0:
                return
            if distance in children:
                node = children[distance]
            else:
                children[distance] = (word, {})
                break

    def search(self, query, max_dist):
        if self.tree is None:
            return []
        candidates = [self.tree]
        results = []
        while candidates:
            node_word, children = candidates.pop()
            distance = Levenshtein.distance(query, node_word)
            if distance <= max_dist:
                results.append((distance, node_word))
            low, high = distance - max_dist, distance + max_dist
            for d, child in children.items():
                if low <= d <= high:
                    candidates.append(child)
        return results


bk_tree = BKTree()
EXACT_WHITELIST = set()
SUSPICIOUS_TLDS = {'.xyz', '.top', '.club', '.win', '.online',
                   '.info', '.gq', '.tk', '.ml', '.ga', '.cf', '.cn'}


def init_and_load_db():
    with app.app_context():
        db.create_all()

        if not WhitelistDomain.query.first():
            initial_data = ["google.com", "vk.com",
                            "yandex.ru", "sberbank.ru", "mail.ru"]
            for d in initial_data:
                db.session.add(WhitelistDomain(domain=d))
            db.session.commit()

        domains = WhitelistDomain.query.all()

        for row in domains:
            d = row.domain.strip().lower()
            EXACT_WHITELIST.add(d)
            bk_tree.add(d)


init_and_load_db()


def normalize_homoglyphs(text):
    mapping = {
        'Ð°': 'a', 'Ñ': 'c', 'Ðµ': 'e', 'Ð¾': 'o', 'Ñ€': 'p', 'Ñ…': 'x', 'Ñƒ': 'y', 'Ðº': 'k', 'Ð¼': 'm',
        'Ð': 'A', 'Ð¡': 'C', 'Ð•': 'E', 'Ðž': 'O', 'Ð ': 'P', 'Ð¥': 'X', 'Ð£': 'Y', 'Ðš': 'K', 'Ðœ': 'M'
    }
    return "".join(mapping.get(char, char) for char in text)


def clean_domain(url):
    try:
        if not url:
            return ""
        url = url.strip().lower()
        url = re.sub(r'^https?://', '', url)
        url = re.sub(r'^www\.', '', url)
        parts = re.split(r'[/?:@]', url)
        if parts:
            return parts[0]
        return url
    except:
        return ""


def is_ip_address(domain):
    try:
        ipaddress.ip_address(domain)
        return True
    except ValueError:
        return False


app.config['MAX_CONTENT_LENGTH'] = 2 * 1024 * 1024


def analyze_url_structure(full_url, domain):
    score = 0
    reasons = []

    if is_ip_address(domain):
        score += 80
        reasons.append("ðŸ“› Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ IP-Ð°Ð´Ñ€ÐµÑÐ° Ð²Ð¼ÐµÑÑ‚Ð¾ Ð´Ð¾Ð¼ÐµÐ½Ð°")
        return score, reasons

    if '@' in full_url:
        score += 60
        reasons.append("ðŸŽ£ ÐžÐ±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½ ÑÐ¸Ð¼Ð²Ð¾Ð» '@' (Ð¿Ð¾Ð¿Ñ‹Ñ‚ÐºÐ° Ð¾Ð±Ð¼Ð°Ð½Ð° URL)")

    for tld in SUSPICIOUS_TLDS:
        if domain.endswith(tld):
            score += 15
            reasons.append(f"ðŸš© ÐŸÐ¾Ð´Ð¾Ð·Ñ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ Ð´Ð¾Ð¼ÐµÐ½Ð½Ð°Ñ Ð·Ð¾Ð½Ð°: {tld}")
            break

    if domain.count('.') > 3:
        score += 20
        reasons.append("ðŸ”— ÐŸÐ¾Ð´Ð¾Ð·Ñ€Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð¼Ð½Ð¾Ð³Ð¾ Ð¿Ð¾Ð´Ð´Ð¾Ð¼ÐµÐ½Ð¾Ð² (4+)")

    return score, reasons


def analyze_typosquatting(current_domain):
    normalized = normalize_homoglyphs(current_domain)

    if normalized in EXACT_WHITELIST:
        if current_domain != normalized:
            return 100, f"ðŸ”¤ Homoglyph attack! ÐŸÐ¾Ð´Ð¼ÐµÐ½Ð° ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð² Ð¿Ð¾Ð´ {normalized}"
        return 0, None

    for legit in EXACT_WHITELIST:
        if normalized.endswith('.' + legit):
            return 0, None

    max_distance = 2
    found = bk_tree.search(normalized, max_distance)

    if found:
        found.sort(key=lambda x: x[0])
        best_dist, best_match = found[0]
        risk = 90 if best_dist == 1 else 70
        return risk, f"âš ï¸ Typosquatting: {best_match} (dist: {best_dist})"

    return 0, None


def analyze_content_optimized(html_raw):
    score = 0
    reasons = []

    if len(html_raw) > 500_000:
        html_raw = html_raw[:500_000]

    soup = BeautifulSoup(html_raw, 'html.parser')
    for tag in soup(["script", "style", "svg", "img", "iframe", "noscript", "video", "audio"]):
        tag.decompose()

    text = soup.get_text(" ", strip=True).lower()

    triggers = {
        'urgent': ['blocked', 'suspended', 'immediately', 'urgent', 'Ð·Ð°Ð±Ð»Ð¾ÐºÐ¸Ñ€Ð¾Ð²Ð°Ð½', 'ÑÑ€Ð¾Ðº', 'ÑÑ€Ð¾Ñ‡Ð½Ð¾', '24 Ñ‡Ð°ÑÐ°', 'Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ'],
        'money': ['card', 'cvv', 'bank', 'payment', 'ÐºÐ°Ñ€Ñ‚Ð°', 'Ð¿Ð»Ð°Ñ‚ÐµÐ¶', 'Ñ€ÐµÐºÐ²Ð¸Ð·Ð¸Ñ‚Ñ‹', 'Ð±Ð°Ð½Ðº', 'ÑÑ€ÐµÐ´ÑÑ‚Ð²Ð°'],
    }

    found_urgency = any(w in text for w in triggers['urgent'])
    found_money = any(w in text for w in triggers['money'])
    has_pass = bool(soup.find('input', {'type': 'password'}))

    if has_pass:
        if found_urgency or found_money:
            score += 70
            reasons.append("ðŸš¨ Ð’Ð²Ð¾Ð´ Ð¿Ð°Ñ€Ð¾Ð»Ñ + Ð¡Ñ€Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ/Ð”ÐµÐ½ÑŒÐ³Ð¸")
        else:
            score += 10
            reasons.append("ðŸ”‘ Ð¤Ð¾Ñ€Ð¼Ð° Ð²Ð²Ð¾Ð´Ð° Ð¿Ð°Ñ€Ð¾Ð»Ñ")
    else:
        if found_money and found_urgency:
            score += 40
            reasons.append(
                "ðŸ“¢ Ð¢ÐµÐºÑÑ‚ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚ ÑƒÐ³Ñ€Ð¾Ð·Ñ‹ Ð¸ Ñ„Ð¸Ð½Ð°Ð½ÑÐ¾Ð²Ñ‹Ðµ Ñ‚Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸Ñ (Ð¡ÐºÐ°Ð¼)")

    return score, reasons


@app.route('/analyze', methods=['POST'])
def analyze():
    try:
        data = request.json
        full_url = data.get('domain', '')
        html = data.get('html', '')

        domain = clean_domain(full_url)
        if not domain:
            return jsonify({"error": "Bad URL"}), 400

        total_score = 0
        all_reasons = []

        url_score, url_reasons = analyze_url_structure(full_url, domain)
        total_score += url_score
        all_reasons.extend(url_reasons)

        ts_score, ts_reason = analyze_typosquatting(domain)
        if ts_score > 0:
            total_score += ts_score
            all_reasons.append(ts_reason)

        if html:
            cont_score, cont_reasons = analyze_content_optimized(html)

            if (ts_score > 0 or url_score >= 50) and cont_score > 0:
                total_score = 100
            else:
                total_score += cont_score
            all_reasons.extend(cont_reasons)

        return jsonify({
            "risk_score": min(total_score, 100),
            "reasons": all_reasons
        })

    except Exception as e:
        return jsonify({"error": str(e)}), 500


if __name__ == '__main__':
    app.run(debug=True, port=5000)
